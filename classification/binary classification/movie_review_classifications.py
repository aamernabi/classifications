# -*- coding: utf-8 -*-
"""Movie Review Classifications.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lw0HQs8gyaIHwDfeC4Y-EzZo5HLAZJUI

# IMDB Movie Reviews Classification (Binary Classification)
"""

# imports
from __future__ import absolute_import, division, print_function, unicode_literals

import numpy as np

import keras
from keras.datasets import imdb

(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)

print(f"training data: ${train_data.shape}")
print(f"training labels: ${train_labels.shape}")

print()

print(f"test data: ${test_data.shape}")
print(f"test labels: ${test_labels.shape}")

print(train_data[2])
print(len(train_data[2]))

train_labels[2]

print(max(max(s) for s in train_data))

"""### Reverse reviews back to english"""

def decode_imdb_review(word_index_dict, item):
  #reverse word_index
  index_word_dict = dict([(value, index) for (index, value) in word_index_dict.items()])
  return ' '.join([index_word_dict.get(i-3, '?') for i in item]) #offset by 3, 0, 1, 2 reserved for "padding", "start of sequence", and "unknown"

review = decode_imdb_review(imdb.get_word_index(), train_data[1432])
print(review)

"""### Prepare the data"""

def vectorize(sequences, dim=10000):
  result = np.zeros((len(sequences), dim))
  for i, seq in enumerate(sequences):
    result[i, seq] = 1
  return result

x_train = vectorize(train_data)
x_test = vectorize(test_data)

y_train = np.asarray(train_labels).astype('float32') 
y_test = np.asarray(test_labels).astype('float32')

from keras.layers import Dense
from keras.models import Sequential


model = Sequential()
model.add(Dense(16, activation='relu', input_shape=(10000,)))
model.add(Dense(16, activation='relu'))
model.add(Dense(1, activation='sigmoid'))

model.compile(
    optimizer='rmsprop',
    loss='binary_crossentropy',
    metrics=['accuracy']
)

#set validation data
x_val = x_train[:10000]
partial_x_trian = x_train[10000:]

y_val = y_train[:10000]
partial_y_train = y_train[10000:]



history.history.keys()

import matplotlib.pyplot as plt

def plotloss(history):
  loss = history.history['loss']
  val_loss = history.history['val_loss']

  epochs_range = range(1, len(loss) + 1)

  plt.plot(epochs_range, loss, 'bo', label='Training Loss')
  plt.plot(epochs_range, val_loss, 'b', label='Validation Loss')
  plt.title('Training ad=nd validation loss')
  plt.xlabel('Epochs')
  plt.ylabel('Loss')
  plt.legend()

  plt.show()

plotloss(history)

def plotaccuracy(history):
  acc = history.history['accuracy']
  val_acc = history.history['val_accuracy']
  epochs_range = range(1, len(acc) + 1)

  plt.plot(epochs_range, acc, 'bo', label='Training Accuracy')
  plt.plot(epochs_range, val_acc, 'b', label='Validation Accuracy')
  plt.title('Training and validation accuracy')
  plt.xlabel('Epochs')
  plt.ylabel('Accuracy')
  plt.legend()

  plt.show()

plotaccuracy(history)

"""From figures it clear, that our models is overfitting."""

model = Sequential([
    Dense(16, activation='relu', input_shape=(10000,)),
    Dense(16, activation='relu'),
    Dense(1, activation='sigmoid')
])

model.compile(
    optimizer='rmsprop',
    loss='binary_crossentropy',
    metrics=['accuracy']
)

history = model.fit(
    partial_x_trian,
    partial_y_train,
    epochs=4,
    batch_size=512,
    validation_data=(x_val, y_val)
)

plotloss(history)
plotaccuracy(history)

model = Sequential([
    Dense(16, activation='relu', input_shape=(10000,)),
    Dense(16, activation='relu'),
    Dense(1, activation='sigmoid')
])

model.compile(
    optimizer='rmsprop',
    loss='binary_crossentropy',
    metrics=['accuracy']
)

model.fit(x_train, y_train, epochs=4, batch_size=512)
result = model.evaluate(x_test, y_test)

result

model.predict(x_test)